Create a node that has an SSD and label it as such.
    kubectl label nodes worker-1 disktype=ssd
Create a pod that is only scheduled on SSD nodes.
    add nodeSelector: disktype: ssd
Create 2 pod definitions: the second pod should be scheduled to run anywhere the first pod is running - 2nd pod runs alongside the first pod.
Create a deployment running nginx version 1.12.2 that will run in 2 pods
    $ kubectl create deployment deploypod --image=nginx:1.12.2 --replicas=2
Scale this to 4 pods.
    $ kubectl scale deployment deploypod --replicas=4
Scale it back to 2 pods.
    $ kubectl scale deployment deploypod --replicas=2
Upgrade this to 1.13.8
    $ kubectl set image deployment deploypod nginx=nginx:1.13.8
Check the status of the upgrade
    $ kubectl rollout status deployment deploypod
How do you do this in a way that you can see the history of what happened?
    $ kubectl rollout history deployment deploypod
Undo the upgrade
    $ kubectl rollout undo deployment deploypod
Expose the service on port 80
    $ kubectl expose deployment deploypod --port=80
Create a pod that uses a scratch disk.
Change the pod to mount a path on the host
Taint a node and run a Jenkins Pod on that specified node only.
    $ kubectl taint nodes node1 key1=value1:NoSchedule , to untain '-' is added at  
      last ($ kubectl taint nodes node1 key1=value1:NoSchedule-)
      add in Pod specs tolerations:
                        - key: "example-key"
                          operator: "Exists"
                          effect: "NoSchedule"
Create a pod that has a liveness check----------------------------------------------> Important
    https://cto.ai/blog/health-checks-readiness-probe-and-pod-state/
Use the utility nslookup to look up the DNS records of the service and pod.
    $ kubectl run nginx-pod --image=nginx --port=80
    $ kubectl expose pod nginx-pod
    $ kubectl run busybox3 --image=busybox:1.28 --rm --restart=OnFailure -ti -- 
      /bin/nslookup nginx-pod > nginx-svc-out.txt

Find which Pod is taking max CPU
    kubectl top pods --sorty-by cpu
List all PersistentVolumes sorted by their name
    kubectl get pv --sorty-by cpu
Create a daemon set
    Like other controllers, DaemonSets manage groups of replicated Pods.

    However, DaemonSet ensures that all or selected Worker Nodes run a copy of       
      Pod  (one-Pod-per-node).
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
            name: fluentd
            namespace: kube-system
        labels:
            k8s-app: fluentd
        spec:
        selector:
            matchLabels:
            name: fluentd
        template:
            metadata:
            labels:
                name: fluentd
            spec:
            containers:
            - name: fluentd
                image: fluentd:latest

Change the update strategy to do a rolling update but delaying 30 seconds between pod updates 
        apiVersion: apps/v1
        kind: Deployment
        metadata:
        name: myapp
        spec:
        replicas: 2
        strategy: 
        type: RollingUpdate
        rollingUpdate:
            maxSurge: 1
            maxUnavailable: 25%
            
        selector:
            matchLabels:
                app: dev
        template:
            metadata:
                labels:
                app: dev
            spec:
            containers: 
            - image: nginx:latest
                name: myapp
                ports:
                - containerPort: 8080
                livenessProbe:
                httpGet:
                    path: /
                    port: 8080
                initialDelaySeconds: 30
        
        https://medium.com/platformer-blog/enable-rolling-updates-in-kubernetes-with-zero-downtime-31d7ec388c81
Create a static pod
        1. login to node
        2. we have to place static pod manifest in that directory where other static pods are placed
        3. kubectl get pods >>> pod-name-[node_name] will be the static pod like etcd-master
        4. sudo cat /var/lib/kubelet/config.yaml >>> see staticPodPath 
        5. cd /etc/kubernetes/manifests >>> create pod file inside it and apply.

Create a busybox container without a manifest. Then edit the manifest.
        kubectl run -i -t busybox --image=busybox
        kubectl edit pod busybox

Create a pod that uses secrets -
        C:\Users\abhishesrivasta6\Desktop\CKA Exam\CKA-Exam\Section 3 - Workloads and Scheduling - 15% 
        Validate:
        ---------
        kubectl exec secret-env-pod -- env
        kubectl exec secret-env-pod -- env | grep SECRET
        
Create a secret
        C:\Users\abhishesrivasta6\Desktop\CKA Exam\CKA-Exam\Section 3 - Workloads and Scheduling - 15% 

Pull secrets from environment variables -- same as question "Create a pod that uses secrets"

Pull secrets from a volume
         C:\Users\abhishesrivasta6\Desktop\CKA Exam\CKA-Exam\Section 3 - Workloads and Scheduling - 15% 
         
Dump the secrets out via kubectl to show it worked
         Validate:for volume------->
            kubectl exec secret-vol-pod -- ls /etc/secret-volume
            kubectl exec secret-vol-pod -- cat /etc/secret-volume/username
            kubectl exec secret-vol-pod -- cat /etc/secret-volume/password
        Validate: for env variables
            kubectl exec secret-env-pod -- env
            kubectl exec secret-env-pod -- env | grep SECRET

Create a job that runs every 3 minutes and prints out the current time.
Create a job that runs 20 times, 5 containers at a time, and prints “Hello parallel world”
Create a horizontal autoscaling group that starts with 2 pods and scales when CPU usage is over 50%.
Create a custom resource definition - CRD
Display it in the API with curl
Create a networking policy such that only pods with the label access=granted can talk to it.
Create a nginx pod and attach this policy to it.
Create a busybox pod and attempt to talk to nginx - should be blocked
Attach the label to busybox and try again - should be allowed
Create a service that references an externalname - https://api.github.com/users/prabhatsharma
Test that this works from another pod
Create a pod that runs all processes as user 1000.
Create a namespace
Run a pod in the new namespace
Put memory limits on the namespace
Limit pods to 2 persistent volumes in this namespace
Write an ingress rule that redirects calls to /foo to one service and to /bar to another
Write a service that exposes nginx on a nodeport
Change it to use a cluster port
Scale the service
Change it to use an external IP
Change it to use a load balancer
Deploy nginx with 3 replicas and then expose a port
Use port forwarding to talk to a specific port